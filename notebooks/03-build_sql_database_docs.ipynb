{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build-sql-database\n",
    "Builds database in SQL por from the CPDOC collection **Antonio Azeredo da Silveira, Ministério das Relações Exteriores (AAS-MRE_**.  \n",
    "\n",
    "The meta-data included are:\n",
    "* doc_id\n",
    "* text body\n",
    "* main language\n",
    "* readability\n",
    "* URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "from langdetect import detect\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set operational system variables\n",
    "Verifies what operational system is being used and creates user-specific variables. Renato = Linux ; Marcelo = nt (Windows)\n",
    "\n",
    "Also sets working folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = os.path.join(\"..\", \"inputs\")\n",
    "outputs = os.path.join(\"..\",\"outputs\")\n",
    "if os.name == 'nt':\n",
    "    encoding_type = 'utf-8'\n",
    "else:\n",
    "    encoding_type = 'ISO-8859-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\pseudo-dropbox\\backups-fgv\\textfiles/textfiles-corrected-regrouped/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create special sorting function\n",
    "* Creates function that list files on different order\n",
    "* It was important to pay atention to files with different numeration, such as :\n",
    "* AAS_mre_onu_1975.01.23_doc_I-A.txt\n",
    "* AAS_mre_onu_1975.01.23_doc_I-6A8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_zero(x):\n",
    "    if x == '': x = '0'\n",
    "    return x\n",
    "def special_sort(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else str(text)\n",
    "    alphanum_key = lambda key: [ convert(to_zero(c)) for c in filter(None, re.split('(\\d)A|A\\d|([A-Z]*)-A?|.txt', key))] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create list of files with special sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(path))]\n",
    "fullpath_list = []\n",
    "fullpath = ''\n",
    "for file in files:\n",
    "    fullpath = path+file\n",
    "    fullpath_list.append(fullpath)\n",
    "fullpath_list = special_sort(fullpath_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-1.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-2.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-3.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-4.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-5.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-6.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-7.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-8.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-9.txt',\n",
       " 'D:\\\\pseudo-dropbox\\\\backups-fgv\\\\textfiles/textfiles-corrected-regrouped/AAS_mre_ag_1973.11.20_doc_I-10.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullpath_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# url data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates list of dossiers and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['d_1974.03.26',\n",
       "  'd_1974.04.23',\n",
       "  'vp_1975.09.03',\n",
       "  'vp_1976.03.30',\n",
       "  'vp_1977.10.31'],\n",
       " ['http://www.fgv.br/cpdoc/acervo/arquivo-pessoal/AAS/textual/documentos-referentes-aos-despachos-de-azeredo-da-silveira-como-ministro-das-relacoes-exteriores-com-o-presidente-da-republica-ernesto-geisel-o-do',\n",
       "  'http://www.fgv.br/cpdoc/acervo/arquivo-pessoal/AAS/textual/documentos-do-tipo-lembretes-utilizados-nos-despachos-do-ministro-das-relacoes-exteriores-azeredo-da-silveira-com-o-presidente-ernesto-geisel-abor',\n",
       "  'http://www.fgv.br/cpdoc/acervo/arquivo-pessoal/AAS/textual/documentos-referentes-a-atuacao-de-azeredo-da-silveira-como-ministro-das-relacoes-exteriores-nas-viagens-do-presidente-ernesto-geisel-a-franca-em-',\n",
       "  'http://www.fgv.br/cpdoc/acervo/arquivo-pessoal/AAS/textual/documentos-referentes-a-atuacao-de-azeredo-da-silveira-como-ministro-das-relacoes-exteriores-na-viagem-do-presidente-ernesto-geisel-ao-japao-em-se',\n",
       "  'http://www.fgv.br/cpdoc/acervo/arquivo-pessoal/AAS/textual/documentos-referentes-a-atuacao-de-azeredo-da-silveira-como-ministro-das-relacoes-exteriores-na-viagem-do-presidente-ernesto-geisel-ao-mexico-e-ao'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = []\n",
    "dossie_url_list = []\n",
    "url_inputs = os.path.join(inputs,\"URLS-AAS-marcelo.csv\")\n",
    "with open(url_inputs, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';')\n",
    "    for row in reader:\n",
    "        dossie = row[1]\n",
    "        if not dossie.startswith('AAS mre'): continue\n",
    "        dossie = re.sub(' ','_',dossie)\n",
    "        dossie = re.sub('AAS_mre_(.*)',r'\\1',dossie)\n",
    "        dossie = re.sub('\\/',r'-',dossie)\n",
    "        dossie_url_list.append(dossie)\n",
    "        url_list.append(row[2])\n",
    "dossie_url_list[:5], url_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# builds sql database\n",
    "Classifies corpus by: id, readability and main_language.\n",
    "\n",
    "Stores metadata in sql. Other meta-data will be placed on dossie table: dossie_id, dossie subject, date (inaccurate)\n",
    "\n",
    "Documents with extremely low readability (less than 30% of readable phrases) are taken out of the database and listed on a file for later analysis, so that we can think of solutions in the future. Those documents are mainly manuscrits, pictures and texts with stains, scratches or drafts.\n",
    "\n",
    "Should change in the future the lang_class column. Maybe to 2 columns: main_language, second_language.\n",
    "\n",
    "For texts with a length of 10 **phrases**, I placed the number -1 on readability because there would be too few observations. Should make a new column with info about **word** length. Instead of the -1, I would have the estimated length and the length info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 % done\n"
     ]
    }
   ],
   "source": [
    "doc_class = []\n",
    "lang_class = 'none'\n",
    "not_readable = []\n",
    "percentil = int(len(fullpath_list)/100)\n",
    "\n",
    "sql_db = os.path.join(inputs, 'cpdoc_as.sqlite')\n",
    "conn = sqlite3.connect(sql_db)\n",
    "cur = conn.cursor()\n",
    "    \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "inserts data into sql database\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "user_input = input(\"Data will be erased and replaced. Continue? Type 'yes' or 'no' on your keyboard: \")\n",
    "if user_input.lower() == 'yes': \n",
    "    cur.execute(\"DROP TABLE IF EXISTS docs\")\n",
    "    cur.execute('''CREATE TABLE IF NOT EXISTS docs \n",
    "               (id VARCHAR(31) PRIMARY KEY, main_language VARCHAR(10), readability DECIMAL(3,2), url LONGTEXT, body LONGTEXT\n",
    "               DEFAULT NULL)''')\n",
    "\n",
    "    ''' iterates through texts '''\n",
    "    for count_doc,txt in enumerate(fullpath_list):\n",
    "\n",
    "        ''' measures completion percentage '''\n",
    "        if count_doc % percentil == 0: \n",
    "            clear_output()\n",
    "            print(int(count_doc/percentil),'% done')\n",
    "    #         if count_doc != 0: break\n",
    "\n",
    "        ''' captures info about date, year, month and ids '''\n",
    "        txt_date = re.sub('.*(19\\d\\d\\.\\d\\d\\.\\d\\d).*', r'\\1', txt)\n",
    "        txt_year = re.sub('.*(19\\d\\d).*', r'\\1', txt)\n",
    "        txt_month = re.sub('.*19\\d\\d\\.(\\d\\d).*', r'\\1', txt)\n",
    "        txt_id = re.sub('.*AAS_mre_(.*).txt', r'\\1', txt)\n",
    "\n",
    "        dossie = re.sub('(.*)_doc_.*', r'\\1', txt_id)\n",
    "        url_index = dossie_url_list.index(dossie)\n",
    "        url = url_list[url_index]\n",
    "\n",
    "        ''' makes analysis in each document '''\n",
    "        with open(txt, 'r', encoding=encoding_type) as f:\n",
    "            txt_body = f.read()\n",
    "\n",
    "            ''' identifies main language and readability of each document '''\n",
    "            text_split = re.split('\\.|\\?|\\:|\\,', txt_body)\n",
    "            pt_count = en_count = es_count = fr_count = de_count = lang_count = total_count = 0\n",
    "            for phrase in text_split:\n",
    "                try: \n",
    "                    if len(re.findall(\"[^\\W\\d]\", phrase)) <= 10: continue\n",
    "                    language = detect(phrase)\n",
    "                    total_count += 1\n",
    "                except: \n",
    "                    continue\n",
    "                if language == 'pt':\n",
    "                    pt_count += 1\n",
    "                if language == 'en':\n",
    "                    en_count += 1\n",
    "                if language == 'es':\n",
    "                    es_count += 1\n",
    "                if language == 'fr':\n",
    "                    fr_count += 1\n",
    "                if language == 'de':\n",
    "                    de_count += 1\n",
    "            lang_count = pt_count + en_count + es_count + fr_count + de_count        \n",
    "            if total_count == 0: readability_ratio = 0\n",
    "            else: readability_ratio = float(lang_count/total_count)\n",
    "            if readability_ratio < 0.3: \n",
    "                not_readable.append(txt)\n",
    "                continue\n",
    "            elif total_count > 10: readability = readability_ratio\n",
    "            else: readability = -1\n",
    "            ''' note: with the criteria, documents might have readability but no lang_class '''\n",
    "            if de_count/total_count > 0.3 and de_count >= 3: \n",
    "                lang_class = 'de'\n",
    "            if fr_count/total_count > 0.3 and fr_count >= 3: \n",
    "                lang_class = 'fr'\n",
    "            if es_count/total_count > 0.3 and es_count >= 3: \n",
    "                lang_class = 'es'\n",
    "            if en_count/total_count > 0.3 and en_count >= 3: \n",
    "                lang_class = 'en'\n",
    "            if pt_count/total_count > 0.3 and pt_count >= 3: \n",
    "                lang_class = 'pt'\n",
    "\n",
    "            ''' inserts data into sql '''\n",
    "            query = \"INSERT INTO docs VALUES (?,?,?,?,?)\"\n",
    "            cur.execute(query, (txt_id, lang_class, readability, url, txt_body))\n",
    "else:\n",
    "    print('Table wasnot created/replaced')\n",
    "        \n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves list of files with too low readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_readable_files = os.path.join(outputs, \"not_readable_files.txt\")\n",
    "\n",
    "with open(not_readable_files, 'w+', encoding='utf-8') as f: \n",
    "    text = f.write(not_readable[0])\n",
    "    text = f.write('\\r\\n')\n",
    "for file in not_readable[1:]:\n",
    "    with open(not_readable_files, 'a+', encoding='utf-8') as f:\n",
    "        text = f.write(file)\n",
    "        text = f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
